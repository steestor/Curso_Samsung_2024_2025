{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Matemáticas para Inteligencia Artificial (II)\n",
    "\n",
    "\n",
    "\n",
    "### 1. Matrices invertibles\n",
    "\n",
    "\n",
    "\n",
    "Recordemos que una matriz $A\\in\\mathcal{M}_{n\\times n}(\\mathbb{R})$, con $n\\in\\mathbb{N}$, es **invertible** si existe otra matriz $B \\in\\mathcal{M}_{n\\times n}(\\mathbb{R})$ tal que $AB=BA=I_n$, la matriz identidad de tamaño $n\\times n$. En este caso, denotamos a $B$ por $A^{-1}$ y diremos que es la **inversa de** $A$. Se puede demostrar fácilmente que la inversa de $A$ existe si, y sólo si, $|A|\\neq 0$. Veamos algunos ejemplos con Python. Trabajemos con la siguiente matriz.\n",
    "\n",
    "$$ A = \\begin{pmatrix} 0&-2&3 \\\\ -4&6&0\\\\ 2&-5&5\\end{pmatrix}.$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numpy import linalg as la"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A=np.array([[0,-2,3],[-4,6,0],[2,-5,5]])\n",
    "print(A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "la.det(A)     # luego A es invertible"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inv=la.inv(A)\n",
    "print(inv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Valores propios, vectores propios y diagonalización de matrices\n",
    "\n",
    "\n",
    "Recordemos que, dada una matriz $A\\in\\mathcal{M}_{n\\times n}(\\mathbb{R})$, un **valor propio de $\\pmb{A}$** (o **autovalor de $\\pmb{A}$**) es un número $\\lambda\\in\\mathbb{C}$ para el cual existe un vector $\\pmb{0}\\neq v\\in\\mathbb{C}^n$, denominado **vector propio de $\\pmb{A}$** (o **autovector de $\\pmb{A}$**), cumpliendo la igualdad $A\\cdot v = \\lambda \\cdot v$. En otras palabras, un vector propio de $A$ es un vector que $A$ transforma en un múltiplo de él, dejando invariante por tanto su dirección. El conjunto formado por todos los valores propios de $A$ se denota por $\\sigma(A)$, y el conjunto de todos los vectores propios de $A$ asociados a $\\lambda$ se denota por $E(\\lambda)$ y forma un subespacio vectorial de $\\mathbb{C}^n$ cuando añadimos al vector nulo.\n",
    "\n",
    "\n",
    "Observemos que la igualdad anterior equivale a $(A-\\lambda\\cdot I)\\cdot v = \\pmb{0}$. Si el rango de $A-\\lambda\\cdot I$ es $n$, entonces dicho sistema tiene como única solución $v=\\pmb{0}$, lo cual no nos interesa. Por tanto, queremos ver cuándo $A-\\lambda\\cdot I$ no tiene rango máximo o, en otras palabras, cuándo $|A-\\lambda\\cdot I|=0$. Así es como se calculan a mano los valores propios. Los vectores propios asociados a $\\lambda$ se pueden calcular a través del núcleo de $A-\\lambda\\cdot I$.\n",
    "\n",
    "\n",
    "Unas propiedades importantes de los valores propios son que $|A|=\\displaystyle\\sum_{\\lambda\\in\\sigma(A)} \\lambda$ y $\\operatorname{traza}(A)=\\displaystyle\\sum_{\\lambda\\in\\sigma(A)} \\lambda$. En particular, existe $A^{-1}$ si, y sólo si, $0\\notin\\sigma(A)$. También es destacable que los vectores propios asociados a valores propios distintos son necesariamente linealmente independientes.\n",
    "\n",
    "\n",
    "Veamos cómo calcular estos elementos con Python. Trabajaremos a modo de ejemplo con la matriz \n",
    "\n",
    "$$A=\\begin{pmatrix} 3&-2&0 \\\\ 4&-1&0 \\\\ 0&0&1\\end{pmatrix}.$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A=np.array([[3,-2,0],[4,-1,0],[0,0,1]])\n",
    "print(A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "autovalores, autovectores = la.eig(A)\n",
    "\n",
    "print(autovalores, \"\\n\")   # j es el número imaginario i.\n",
    "print(np.round(autovectores,3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.round(np.prod(autovalores),3))\n",
    "print(np.round(la.det(A),3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.round(np.sum(autovalores),3))\n",
    "print(np.round(np.trace(A),3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En el caso de que la matriz $A$ tenga **$n$ vectores propios linealmente independientes**, y sólamente en ese caso, la matriz $A$ se puede **diagonalizar**: se cumple que $P^{-1}\\cdot A \\cdot P=D$ donde $D$ es una matriz diagonal formada por los autovalores de $A$ y $P$ es una matriz cuya columna $i$-ésima es vector propio asociado al valor propio en la entrada diagonal $i$-ésima de $D$. Notemos que $P$ es invertible porque sus columnas son linealmente independientes por hipótesis, luego tiene rango $n$.\n",
    "\n",
    "\n",
    "En particular, si $A$ tiene $n$ valores propios distintos, entonces como hemos comentado anteriormente tendrá $n$ vectores propios linealmente independientes y, por tanto, será siempre diagonalizable. Luego la matriz del ejemplo anterior (que tenía 2 autovalores complejos conjugados y uno real) es diagonalizable seguro."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "la.matrix_rank(autovectores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.round(la.inv(autovectores)@A@autovectores,3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Esta igualdad tiene numerosas aplicaciones. Por ejemplo, como $A=P\\cdot D \\cdot P^{-1}$, entonces resulta _sencillo_ calcular potencias de $A$, pues $A^k=P\\cdot D^k\\cdot P^{-1}$, siendo $D^{k}$ fácil de calcular al ser $D$ una matriz diagonal.\n",
    "\n",
    "\n",
    "Si $A$ no tiene $n$ vectores propios distintos, entonces no será diagonalizable. Por ejemplo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A=np.array([[1,1,0],[0,1,0],[0,0,1]])\n",
    "autovalores, autovectores=la.eig(A)\n",
    "print(autovalores, \"\\n\")\n",
    "print(np.round(autovectores,3), \"\\n\")\n",
    "print(la.matrix_rank(autovectores))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Proposición.** Si $A$ es simétrica (i.e. $A=A^t$), entonces sus valores propios son todos reales.\n",
    "\n",
    "(Para demostrarlo, basta con ver que $A^2v=\\lambda^2 v$ y tomar el cuadrado de la norma de $Av$.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Descomposición en Valores Singulares (SVD)\n",
    "\n",
    "\n",
    "La diagonalización de matrices tiene sentido cuando hablamos de matrices cuadradas. Pero, en general, si $A\\in\\mathcal{M}_{n\\times m}(\\mathbb{C})$, también existe otro tipo de descomposición denominada **descomposición en valores singulares** (SVD por sus siglas en inglés), la cual descompone $A=U\\cdot \\Sigma\\cdot V$. En el caso que $A$ sea real (que será el que vamos a trabajar) se cumple que:\n",
    "\n",
    "- $U$ y $V$ son matrices ortogonales (no únicas) de dimensiones $n\\times n$ y $m\\times m$, respectivamente.\n",
    "- $\\Sigma$ es una matriz $n\\times m$ cuyas entradas fuera de la diagonal principal son nulas.\n",
    "- $A^t A$ tiene valores propios reales y positivos (por ser una matriz simétrica y definida positiva).\n",
    "- Si $A$ tiene rango $r$, entonces $\\sigma(A^tA)$ contiene $r$ valores propios no nulos $\\{\\sigma_1^2,...,\\sigma_r^2\\}$; sus raíces cuadradas se denominan **valores singulares de $\\pmb{A}$**.\n",
    "- Si suponemos que están ordenados $\\sigma_1^2\\geq \\cdots \\geq \\sigma_r^2 > 0$, entonces las entradas de la diagonal principal de $\\Sigma$ son $\\{\\sigma_1,...,\\sigma_r,0,...,0\\}$.\n",
    "\n",
    "\n",
    "Si nos fijamos en la estructura de estas matrices, tenemos lo siguiente:\n",
    "\n",
    "\n",
    "\\begin{eqnarray*} A&=&U\\cdot \\Sigma \\cdot V \\\\&=& \\begin{pmatrix}  &  &  \\\\ \\vec{u_1} & \\cdots & \\vec{u_n} \\\\  &  &  \\end{pmatrix} \\cdot \\begin{pmatrix} \\sigma_1 & 0 & \\cdots \\\\ 0 & \\sigma_2 & \\cdots \\\\ \\vdots & \\vdots & \\ddots \\end{pmatrix} \\cdot \\begin{pmatrix}  & \\vec{v_1} &  \\\\  & \\vdots &  \\\\  & \\vec{v_m} &  \\end{pmatrix} \\\\\\ &=& \\begin{pmatrix}  &  &  \\\\ \\sigma_1\\cdot \\vec{u_1} & \\cdots & \\sigma_n\\cdot\\vec{u_n} \\\\  &  &  \\end{pmatrix} \\cdot \\begin{pmatrix}  & \\vec{v_1} &  \\\\  & \\vdots &  \\\\  & \\vec{v_m} &  \\end{pmatrix} \\\\ &=&  \\sigma_1\\cdot \\vec{u_1}\\cdot \\vec{v_1} + \\cdots + \\sigma_r\\cdot \\vec{u_r}\\cdot \\vec{v_r}+0\\cdot\\vec{u_{r+1}}\\cdot \\vec{v_{r+1}}+\\cdots \\\\ &=& \\displaystyle\\sum_{i=1}^{r} \\sigma_i\\cdot \\vec{u_i}\\cdot\\vec{v_i}, \\end{eqnarray*}\n",
    "\n",
    "donde $\\vec{u_i}$ y $\\vec{v_i}$ son vectores unitarios columna y fila, respectivamente, que son autovectores de $AA^t$ y $A^tA$. Como sólamente hay $r$ valores singulares no nulos, entonces a veces se toman $U$, $\\Sigma$ y $V$ de dimensiones $n\\times r$, $r\\times r$ y $r\\times m$, respectivamente, y por ello en el sumatorio anterior aparece el índice $1\\leq i \\leq r$. Esto significa que **las primeras columnas de $\\pmb{U}$ y las primeras filas de $\\pmb{V}$ contribuyen más en la construcción de $\\pmb{A}$** que el resto. De hecho, se tiene el siguiente teorema:\n",
    "\n",
    "\n",
    "**Teorema.** Sea $A\\in\\mathcal{M}_{n\\times m}(\\mathbb{R})$. Si $A_k$ es la descomposición SVD de $A$ utilizando los mayores $k$ valores singulares, entonces $A_k$ es la solución del problema $\\operatorname{min} ||A-\\hat{A}||_F$ bajo la restricción $\\operatorname{rango}(\\hat{A})=k$, donde $||M||_F=\\operatorname{traza}(M^t\\cdot M)$.\n",
    "\n",
    "\n",
    "Este teorema tiene múltiples aplicaciones, por ejemplo en la **compresión de imágenes**. \n",
    "\n",
    "Veamos cómo obtener esta descomposición directamente utilizando Python. Consideremos la matriz $A=\\begin{pmatrix} 1&0&0&0&2\\\\0&0&3&0&0\\\\0&0&0&0&0\\\\ 0&2&0&0&0 \\end{pmatrix}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A=np.array([[1,0,0,0,2],[0,0,3,0,0],[0,0,0,0,0],[0,2,0,0,0]])\n",
    "print(A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "U,S,V=la.svd(A)\n",
    "print(U,\"\\n\")\n",
    "print(S,\"\\n\")\n",
    "print(V)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "U@np.diag(S)@V"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Si nos fijamos, las dimensiones no cuadran para hacer el producto de la descomposición. Esto se debe a que `np.diag(S)` nos devuelve por defecto una matriz simétrica, cuando nuestra $\\Sigma$ debe tener dimensión $4\\times 5$ (i.e. la misma dimensión que $A$).\n",
    "\n",
    "Lo podemos arreglar con el argumento extra `full_matrices=False`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "U,S,V=la.svd(A, full_matrices=False)\n",
    "print(U,\"\\n\")\n",
    "print(S,\"\\n\")\n",
    "print(V)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.round(U@np.diag(S)@V,3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Veamos que, efectivamente, el sumatorio anterior con los 3 valores singulares ya nos proporciona $A$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print((S[0:3]*U[:,0:3])@V[0:3,:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A continuación, vamos a definir una función que, dada la descoposición SVD de una matriz, nos la aproxime en función de $k$ valores singulares que deseemos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def aprox_SVD(U,S,V,k):\n",
    "    return (S[0:k]*U[:,0:k])@V[0:k,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"A1 \\n\", aprox_SVD(A,S,V,1), \"\\n\")\n",
    "print(\"A2 \\n\", aprox_SVD(A,S,V,2), \"\\n\")\n",
    "print(\"A3 \\n\", aprox_SVD(A,S,V,3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Compresión de imágenes\n",
    "\n",
    "\n",
    "La clave para trabajar matemáticamente con imágenes es la siguiente: **una imagen en blanco y negro es una matriz** donde el valor de **cada entrada te determina la intensidad del píxel** correspondiente. Veamos un ejemplo de cómo transformar una imagen en color a blanco y negro, y posteriormente cómo transformarla en una matriz de este tipo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "foto = Image.open('/Users/vmos/Library/CloudStorage/OneDrive-UPV/Curso IA (Samsung)/Apuntes VS (04.2024)/firenze.png')\n",
    "foto_BN = foto.convert('L')   # Luminance\n",
    "plt.figure(figsize=(10, 13))\n",
    "plt.imshow(foto_BN, cmap='gray');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "foto_mat=np.asarray(foto_BN)\n",
    "print(foto_mat.shape)\n",
    "plt.figure(figsize=(10, 13))\n",
    "plt.imshow(foto_mat, cmap='gray');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora comprimiremos dicha imagen a través de una aproximación de su matriz asociada, utilizando su descomposición SVD. Veamos primero cuál es el rango de la matriz, para ver \"hasta dónde\" podemos aproximar la matriz."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "la.matrix_rank(foto_mat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Veamos cómo quedaría la imagen con una aproximación que coja solamente los 10 primeros valores singulares."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[Umat,Smat,Vmat]=la.svd(foto_mat, full_matrices=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "foto_mat_10=aprox_SVD(Umat,Smat,Vmat,10)\n",
    "plt.figure(figsize=(10, 13))\n",
    "plt.title(\"k=10\")\n",
    "plt.imshow(foto_mat_10, cmap='gray');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "foto_mat_100=aprox_SVD(Umat,Smat,Vmat,100)\n",
    "plt.figure(figsize=(10, 13))\n",
    "plt.title(\"k=100\")\n",
    "plt.imshow(foto_mat_100, cmap='gray');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Con los 100 primeros valores singulares ya podemos observar bastante bien la imagen original. Lo destacable es que para calcular esta aproximación solamente hemos necesitado conocer $100\\cdot 4032+100\\cdot3024+100$ valores (los de las primeras 100 columnas de $U$, los de las primeras 100 filas de $V$ y los $100$ primeros valores singulares), mientras que para la foto original necesitábamos conocer los $4032\\cdot 3024$ valores de su matriz asociada. \n",
    "\n",
    "**¡Esto significa que solamente estamos utilizando el $\\dfrac{100\\cdot \\:4032+100\\cdot 3024+100}{4032\\cdot \\:3024}=5.78\\%$ de la información original!**\n",
    "\n",
    "\n",
    "**Ejercicio.** Calcular la aproximación 500 e indicar el porcentaje de información requerido (alrededor del 29%).\n",
    "\n",
    "\n",
    "Para realizar una aproximación de una imagen en color podemos utilizar la misma idea, pero con las tres matrices asociadas a la imagen (RGB)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
