{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aplicación de Máquinas de Vectores de Soporte (SVM) para clasificación, utilizando el conjunto de datos Iris. El objetivo principal es optimizar los hiperparámetros del modelo SVM utilizando diferentes kernels (RBF y polinomial) y evaluar su rendimiento."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Classification with SVM:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "from sklearn.model_selection import train_test_split,GridSearchCV\n",
    "from sklearn.svm import SVC\n",
    "from sklearn import metrics\n",
    "from sklearn.datasets import load_iris\n",
    "warnings.filterwarnings(action='ignore')                  # Turn off the warnings.\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.1. Read in data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = load_iris()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['sepal length (cm)', 'sepal width (cm)', 'petal length (cm)', 'petal width (cm)']\n"
     ]
    }
   ],
   "source": [
    "# Explanatory variables.\n",
    "X = data['data']\n",
    "columns = list(data['feature_names'])\n",
    "print(columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['setosa', 'versicolor', 'virginica']\n"
     ]
    }
   ],
   "source": [
    "# Response variable.\n",
    "Y = data['target']\n",
    "labels = list(data['target_names'])\n",
    "print(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.3, random_state=1234)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2. SVM hyperparameter optimization (RBF kernel):"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "C     : Penalty parameter. <br>\n",
    "gamma : kernel parameter ($\\gamma$).\n",
    "\n",
    "C and gamma are hyperparameters of the Support Vector Machine (SVM) classifier in Scikit-learn, specifically for kernels such as 'rbf', 'poly' and 'sigmoid'.\n",
    "\n",
    "1. 'C' is the Regularization parameter. The strength of the regularization is inversely proportional to C. This hyperparameter helps to avoid overfitting by maintaining a trade-off between achieving a low training error and a low testing error. A smaller value of C creates a wider margin, but more margin violations (more data points will fall into the margin). A larger value of C creates a narrower margin because it doesn't want any violations, possibly leading to overfitting.\n",
    "\n",
    "2. 'gamma' defines how far the influence of a single training example reaches, with low values meaning 'far' and high values meaning 'close'. In other words, with low gamma, points far away from plausible seperation line are considered in calculation for the seperation line.\n",
    "\n",
    "These hyperparameters can be fine-tuned to achieve the most accurate and generalized model possible. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.02 0.04 0.06 0.08 0.1  0.12 0.14 0.16 0.18 0.2  0.22 0.24 0.26 0.28\n",
      " 0.3  0.32 0.34 0.36 0.38]\n"
     ]
    }
   ],
   "source": [
    "C_grid = 0.02*np.arange(1,20)\n",
    "print(C_grid)\n",
    "gamma_grid = 0.02*np.arange(1,50)\n",
    "parameters = {'C': C_grid, 'gamma' : gamma_grid}\n",
    "\n",
    "# Q: Optimize the previous parameters (Grid Search CV with SVC as estimator (kernel = rbf\"), \n",
    "# parameters, cv = 10 and n_jobs = -1 for using all the CPU cores)\n",
    "gridCV = GridSearchCV(nSVC(kernel=\"rbf\"), param_grid=parameters, cv=10, n_jobs=-1)\n",
    "gridCV.fit(X_train, Y_train)\n",
    "best_C = gridCV.best_params_['C']\n",
    "best_gamma = gridCV.best_params_['gamma']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM best C : 0.2\n",
      "SVM best gamma : 0.78\n"
     ]
    }
   ],
   "source": [
    "print(\"SVM best C : \" + str(best_C))\n",
    "print(\"SVM best gamma : \" + str(best_gamma))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM best accuracy : 0.978\n"
     ]
    }
   ],
   "source": [
    "# Q: Instantiate, test and predict with the best C and gamma.\n",
    "# Optimización de hiperparámetros para el kernel RBF:\n",
    "SVM_best = SVC(kernel=\"rbf\", C=best_C, gamma=best_gamma)\n",
    "SVM_best.fit(X_train, Y_train)\n",
    "Y_pred = SVM_best.predict(X_test)\n",
    "print( \"SVM best accuracy : \" + str(np.round(metrics.accuracy_score(Y_test,Y_pred),3)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.3. SVM hyperparameter optimization (Polynomial kernel):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "C_grid = 0.0001*np.arange(1,30)\n",
    "gamma_grid = 0.01*np.arange(1,30)\n",
    "parameters = {'C': C_grid, 'gamma' : gamma_grid}\n",
    "\n",
    "\n",
    "# Q: Optimize the previous parameters (Grid Search CV with SVC as estimator (kernel = poly\"), \n",
    "# parameters, cv = 10 and n_jobs = -1 for using all the CPU cores)\n",
    "# Optimización de hiperparámetros para el kernel polinomial:\n",
    "gridCV = GridSearchCV(SVC(kernel=\"poly\"), param_grid=parameters, cv=10, n_jobs=-1)\n",
    "gridCV.fit(X_train, Y_train)\n",
    "best_C = gridCV.best_params_['C']\n",
    "best_gamma = gridCV.best_params_['gamma']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM best C : 0.0007\n",
      "SVM best gamma : 0.27\n"
     ]
    }
   ],
   "source": [
    "print(\"SVM best C : \" + str(best_C))\n",
    "print(\"SVM best gamma : \" + str(best_gamma))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM best accuracy : 0.956\n"
     ]
    }
   ],
   "source": [
    "# Q: Instantiate, test and predict with the best C and gamma.\n",
    "SVM_best = SVC(kernel=\"poly\", C=best_C, gamma=best_gamma)\n",
    "SVM_best.fit(X_train, Y_train)\n",
    "Y_pred = SVM_best.predict(X_test)\n",
    "print( \"SVM best accuracy : \" + str(np.round(metrics.accuracy_score(Y_test,Y_pred),3)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Kernel RBF (Radial Basis Function):\n",
    "SVM best accuracy : 0.978\n",
    "Este resultado indica que el modelo SVM con kernel RBF, después de la optimización de hiperparámetros, logró una precisión del 97.8% en el conjunto de prueba. Esto significa que el modelo clasificó correctamente el 97.8% de las muestras del conjunto de prueba.\n",
    "Kernel Polinomial:\n",
    "SVM best accuracy : 0.956\n",
    "Este resultado muestra que el modelo SVM con kernel polinomial, tras la optimización de hiperparámetros, alcanzó una precisión del 95.6% en el conjunto de prueba.\n",
    "\n",
    "Conclusiones:\n",
    "\n",
    "Ambos modelos tienen un rendimiento excelente, con precisiones superiores al 95%. Esto sugiere que el conjunto de datos Iris es bastante separable y que SVM es un método apropiado para este problema de clasificación.\n",
    "El kernel RBF superó ligeramente al kernel polinomial en este caso específico (97.8% vs 95.6%). Esto es bastante común, ya que el kernel RBF suele ser más versátil y eficaz en una amplia gama de problemas.\n",
    "La diferencia de rendimiento entre los dos kernels es relativamente pequeña (2.2 puntos porcentuales). Esto sugiere que ambos kernels son capaces de capturar bien la estructura de los datos.\n",
    "El alto rendimiento de ambos modelos indica que probablemente no hay mucho sobreajuste, ya que las precisiones son muy altas en el conjunto de prueba.\n",
    "Dado que el conjunto de datos Iris tiene solo 150 muestras, es posible que la diferencia observada entre los dos kernels no sea estadísticamente significativa. Se necesitaría un análisis más detallado (por ejemplo, validación cruzada repetida) para confirmar si la diferencia es consistente.\n",
    "El proceso de optimización de hiperparámetros fue efectivo para ambos kernels, permitiendo obtener modelos con alto rendimiento.\n",
    "\n",
    "En resumen, ambos modelos SVM (con kernel RBF y polinomial) son muy eficaces para clasificar el conjunto de datos Iris, con el kernel RBF mostrando un ligero mejor rendimiento en este caso particular. La elección entre uno u otro en una aplicación práctica podría depender de factores adicionales como la velocidad de entrenamiento y predicción, la interpretabilidad del modelo, o el rendimiento en validación cruzada."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
