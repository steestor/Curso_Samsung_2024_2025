{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e77b48d0-bcad-4bd3-9c22-630b5f8309a0",
   "metadata": {},
   "source": [
    "### 1. Algoritmo de Random Forest en Python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9c7777b3-09bd-4054-a10d-5f3c3cbabee0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicciones del modelo: [1 0 2 1 1 0 1 2 1 1 2 0 0 0 0 1 2 1 1 2 0 2 0 2 2 2 2 2 0 0 0 0 1 0 0 2 1\n",
      " 0 0 0 2 1 1 0 0]\n"
     ]
    }
   ],
   "source": [
    "# Importar las bibliotecas necesarias\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, precision_score, recall_score, f1_score\n",
    "from sklearn.datasets import load_iris\n",
    "\n",
    "# Cargar el conjunto de datos Iris\n",
    "iris = load_iris()\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "\n",
    "# Dividir los datos en conjunto de entrenamiento y prueba\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Crear un clasificador Random Forest\n",
    "clf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "\n",
    "# Entrenar el modelo\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# Hacer predicciones\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "# Mostrar las predicciones\n",
    "print(\"Predicciones del modelo:\", y_pred)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "988e108c-f591-4958-8476-187b343544c2",
   "metadata": {},
   "source": [
    "### 2. Matriz de Confusión"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d67d7274-a670-4343-bf8a-21f754363bcc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matriz de Confusión:\n",
      " [[19  0  0]\n",
      " [ 0 13  0]\n",
      " [ 0  0 13]]\n"
     ]
    }
   ],
   "source": [
    "# Calcular la matriz de confusión\n",
    "conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "# Mostrar la matriz de confusión\n",
    "print(\"Matriz de Confusión:\\n\", conf_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad79f8f3-5b4e-4114-8f93-821d46dd622a",
   "metadata": {},
   "source": [
    "Esto te mostrará cuántas predicciones del modelo fueron correctas (verdaderos positivos y verdaderos negativos) y cuántos errores cometió (falsos positivos y falsos negativos)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50a3adbd-192d-4e77-bab6-2582605e9549",
   "metadata": {},
   "source": [
    "### 3. Precisión, Recall y F1 Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1d36e45a-f944-4549-9195-da47a8dbd451",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precisión: 1.0\n",
      "Recall: 1.0\n",
      "F1 Score: 1.0\n"
     ]
    }
   ],
   "source": [
    "# Calcular la precisión, recall y F1 score\n",
    "precision = precision_score(y_test, y_pred, average='weighted')\n",
    "recall = recall_score(y_test, y_pred, average='weighted')\n",
    "f1 = f1_score(y_test, y_pred, average='weighted')\n",
    "\n",
    "# Mostrar las métricas\n",
    "print(\"Precisión:\", precision)\n",
    "print(\"Recall:\", recall)\n",
    "print(\"F1 Score:\", f1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ff47a15-1f32-4f09-9c09-ebd61cbf9fbd",
   "metadata": {},
   "source": [
    "average='weighted': Es útil cuando trabajas con clases desbalanceadas, ya que pondera cada clase de acuerdo a su tamaño. Otras opciones incluyen 'macro' (calcula la métrica para cada clase y promedia el resultado) y 'micro' (considera el total de verdaderos/falsos positivos y negativos a nivel global)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c39a112-6384-4b80-b603-30b747eb7d17",
   "metadata": {},
   "source": [
    "### 4. Evaluación del Modelo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "338fa82d-10d5-4091-b9b7-351f5f7e0d06",
   "metadata": {},
   "source": [
    "Una vez que has entrenado el modelo y obtenido las métricas, puedes evaluar el rendimiento:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ad7b59f5-a072-4d9d-af5e-497a8b598f23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precisión del modelo (Accuracy): 1.0\n",
      "Precisión (Precision): 1.0\n",
      "Sensibilidad (Recall): 1.0\n",
      "F1 Score: 1.0\n"
     ]
    }
   ],
   "source": [
    "# Evaluación general del modelo\n",
    "accuracy = clf.score(X_test, y_test)  # Accuracy general del modelo\n",
    "\n",
    "print(\"Precisión del modelo (Accuracy):\", accuracy)\n",
    "print(\"Precisión (Precision):\", precision)\n",
    "print(\"Sensibilidad (Recall):\", recall)\n",
    "print(\"F1 Score:\", f1)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
